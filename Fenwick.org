#+title: Fenwick Tree in Haskell
#+author: Parsa Alizadeh

* Generic Fenwick Tree

Fenwick tree are known to be really fast. The important tricks are using arrays and indices directly
to locate nodes in the tree. To implement this I need to use ~IO~ or ~ST~ monad, using either
~Array~, ~Vector~, or ~PrimArray~.

** Issues with generic data types

I want the user be able to create a ~newtype~ over ~Int~ for example, and define their own ~Monoid~
or ~Semigroup~ instance for it. The fact that the underlying type is ~Int~ means the operations can
be done in an unboxed array. The problem is that unboxed array do not work with new types very
easily.

One workaround was to use GeneralizedNewTypeDeriving, but after researching, it seems that GND only
works when the type role is nominal, and in unboxed array implementations they are not nominal. (See
[[https://gitlab.haskell.org/ghc/ghc/-/issues/9220][ghc issue #9220]]).

** Prototype using boxed arrays

I've done implementing ~FenIOArray~. It passes manual tests but needs stress testing and
benchmarking. I want to see the bottlenecks too.

Function names are not great and need to be changed.

Semantic of specifying a range is not defined well. We can go with [l, r) or [1, r) in the case of
prefixes or the full range [l, r] and [1, r].

Haskell indices help for 1-based trees in Fenwick implementation. I want to see if they have any
runtime overhead.

Monoids in Fenwick should be commutative. This means ~[]~ or ~First~ are not applicable to fenwicks.

Need to think about monoids of sum modulo m, or products modulo p. They are mathematically groups,
but not straightforward monoids and groups in Haskell. I want to make sure when the modulo is
constant the compiler should optimize it.

See [[https://en.wikipedia.org/wiki/Fenwick_tree][Wikipedia Fenwick tree]]. Also there maybe functions for least significant bit in ~Data.Bits~.

** Coercible Array
<2025-03-14 Fri>

I come with an idea that I haven't seen anywhere. The idea is to represent coercible arrays in a
newtype.

#+begin_src haskell
  newtype ArrayC array rep ix elem = ArrayC (array ix rep)

  instance (IArray array rep, Coercible rep elem) => IArray (ArrayC array rep) elem where { ... }

  instance (Monad m, MArray array rep m, Coercible rep elem) => MArray (ArrayC array rep) elem m where { ... }
#+end_src

Why do I like this idea?
1. I thought implementing ~IArray~ and ~MArray~ would be impossible, but I can compile the above
   code. I think the main reason is using a newtype rather that trying to implement unboxed versions
   directly for newtypes. I own the newtype, so I can implement the instances!
2. There is nothing about boxed or unboxed in the code. This is supports any kind of newtypes for
   any kind of arrays. Although arrays are limited to ~ST~ and ~IO~ arrays mostly, but the idea can
   be applied if it was needed.
3. I didn't benchmark it yet, but I think the overhead of coerce is zero. So the unboxed arrays of
   newtypes have the same speed as the underlying arrays.
4. It is much simpler than ~Data.Vector~.

Why didn't I see this idea sooner?
1. One limiting reason of this idea is using tuples. Tuples are necessary for data structures too.
   Although I haven't seen any Fenwicks with complex data types, it might happened theoretically, and
   ~Data.Vector~ somehow supports it while unboxed ~IO~ and ~ST~ have no support for pairs or
   tuples.
2. Maybe I was ignorant. See [[https://hackage.haskell.org/package/vector-0.13.2.0/docs/Data-Vector-Unboxed.html#t:As][Data.Vector As]] newtype.

I implement instances manually, although I would assume there is an easier way with DerivingVia or
other similar extensions. Also I could use coerce, but they need lots of type application.

It is possible to implement arrays for Enums too (for example a sum type in Haskell can be written
as an unboxed array).

** Modulo type
<2025-03-15 Sat>

I found [[https://stackoverflow.com/questions/39674555/haskell-how-to-write-a-monoid-instance-for-something-that-depends-on-paramete][a question]] on stackoverflow that helped to implement ~Modulo~ type. The mod is in the type
and the compiler should be able to optimize it. Another good thing is that ~GHC.TypeLits~ keeps the
number as a ~Word~ instead of ~S (S (...))~ which is ideal in this case.

I also implemented ~SumMod~ and ~ProductMod~ seperately but realized that ~Sum~ and ~Product~ are
enough! The only exception is ~Group~ instance for ~Product (Modulo a m)~ which assumes ~m~ is
prime.

Another assumption that I didn't think about how to represent is the fact that monoids and groups
defined for Fenwick must be abelian and commutative. I can restrict using general Monoids by
implementing a dummy instance ~AbelianMonoid~ and implement valid instances.

** Profiling
<2025-03-16 Sun>

I tried to compare a C++ implementation with my own. Both of the files are self-contained (in theory
they can be compiled statically, but currently the only way I can compile Haskell is with
~-dynamic~). Both of them are compiled with ~-O2~. The core implementation of Fenwick is completely
the same with C++ and Haskell (Haskell allows more abstraction by the way). For a brief moment I
thought Haskell version is faster, but I was wrong.

C++ version takes a long time on IO, but with scanf/printf or ~sync_with_stdio(false)~ it is more
than 2x faster than Haskell version. If I don't apply any IO optimization to C++, it takes about 20%
more than Haskell version.

This is actually interesting. The only IO optimization in Haskell is with ~readInts~ function. This
is only parsing number using ~ByteString~ library, and there are no explicit optimization to
buffering or similar topics.

Also, profiling helped me to test the code too. It passes the the testcases and outputs the same as
the C++ version.

I used ~perf~ to see the bottlenecks. In C++ bottleneck is correctly within the Fenwick code, while
in Haskell about 15% of runtime is in the base library. This might indicate there are more IO
optimization in Haskell than I know about.

*** ByteString

I did notice in ~perf~ that a chunk of time is related to ~show~ function. It did actually make
sense! I changed every ~print~ to a ByteString version ~printInt~:

#+begin_src haskell
printInts :: [Int] -> IO ()
printInts xs = hPutBuilder stdout (go <> char7 '\n') where
  go = mconcat . intersperse (char7 ' ') . map intDec $ xs

printInt :: Int -> IO ()
printInt x = hPutBuilder stdout (intDec x <> char7 '\n')
#+end_src

The ratio between C++ and Haskell is very close to 2x. I see with ~perf~ that the main bottleneck is
at Fenwick, but some of it is related to ~modifyArray~ which worries me. There are also bound
checking in haskell that might improve the runtime.

*** unsafeRead and unsafeWrite

They have very minimal effect on the runtime, probably less than 10%.

Update: My experiment was wrong. I didn't check the edge cases, which
led to believe that the code is correct. I didn't try to profile code
with correct unsafeRead, but I think the speed improvement won't be
more than 10%, which is the number I said above.

** Haskell library
<2025-03-19 Wed>

I started a haskell library using ~cabal~. The contents of the original files are now separated in
multiple modules.

+ ~Data.Group~
+ ~Data.Modulo~
+ ~Data.Array.ArrayC~ and ~Data.Vector.VectorC~
+ ~Data.Fenwick.Array~

I will implement the vector version of Fenwick later. The current issue is that a unique
~FenwickLike~ typeclass seems impossible with vectors.

** Lowerbound
<2025-03-21 Fri>

It was actually simple all along. [[https://en.wikipedia.org/wiki/Fenwick_tree][Wikipedia]] explains different representations of the Fenwick Tree,
and search tree is the one that allows ~lowerbound~.

I find out there is no restriction of Group needed (the Wikipedia explanation, which is the Knuth
explanation, needs extra operations). It is possible to stay within Monoid. I don't think there is
any performance downside to this change. The only restriction is that partial sums should be
sorted. I do not know if there is a better law that is equivalent to this. I believe they are all
more restrictive and disallow some perfect use cases of Fenwick trees.

** Extensive Input Optimization
<2025-03-25 Tue>

My theory was that a lot of time is wasted for IO in Haskell version. To support this theory, I ran
~perf~ on both C++ and Haskell, and find the difference between the percentage used by Fenwick tree
in C++ and in Haskell. Assuming they are implementing the same logic and should run in a similar
time, That difference means a waste on sections other than Fenwick.

To my surprise, I could not find a lot about IO performance on the Internet. There was an old blog
that I stole the ~map (fst . fromJust . C.readInt) . C.words <$> C.getLine~. I find [[https://stackoverflow.com/questions/43570129][this
stackoverflow question]] that experimented with some other methods. There was ~unfoldr go <$>
C.getLine~ trick (implemented in [[profile/LibraryV1.hs]]) that had some performance gain. The most
performance gain happened with parsing integers in ~C.getContents~. I implemented two different
approach to this: one of them is [[profile/LibraryV2.hs]] which uses ~StateT~ to manage the buffer.
Another one was to use ~IORef~ to store ~ByteString~, and possibly avoid a lot of ~lift~ s in the
code. Both of these have similar performance, and their runtime is really close to the C++ version.
I liked the ~IORef~ version more. It is small and self-contained, and can be easily inserted to any
pre-existing code I had. I tested it for one of my submissions in Codeforces, and the runtime went
down from 700 ms to 500 ms.

I tried similar tricks with stdout, but could not find any gain with that. I now believe that it is
much better to output as fast as possible. Keeping builders in memory has no benefit.  It is also
possible that Haskell might need a fast writer (inverse of parser?) to handle large number of
integers.

I am happy with the final result. Although the improvements here have no connections with Fenwick
trees, they allow more informative experiments.
